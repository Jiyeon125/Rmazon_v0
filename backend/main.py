from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
from pandas import DataFrame
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import text
import os
import shutil
from typing import List, Optional, Dict, Any
import numpy as np
import joblib
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from scipy.sparse import csr_matrix

# --- Pydantic ëª¨ë¸ ì •ì˜ ---
# ìš”ì²­ ë³¸ë¬¸ì˜ ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class Keyword(BaseModel):
    word: str
    count: int

class PredictionRequest(BaseModel):
    price: float
    category: str

# ì‘ë‹µ ë³¸ë¬¸ì˜ ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class PredictionResponse(BaseModel):
    predicted_star: float
    predicted_review_count: float
    price_percentile: float
    review_count_percentile: float
    rating_percentile: float

class SimilarityRequest(BaseModel):
    description: str
    price: float
    discount_percentage: float
    category: str

class Product(BaseModel):
    product_id: str
    product_name: str

class ReviewAnalysis(BaseModel):
    positive_percentage: float
    negative_percentage: float
    neutral_percentage: float
    top_positive_keywords: List[str]
    top_negative_keywords: List[str]

class ProductInfo(BaseModel):
    product_id: str
    product_name: str
    category: str
    price: float
    review_count: int
    review_analysis: ReviewAnalysis

class SimilarProduct(ProductInfo):
    similarity: float

class SimilarityResult(BaseModel):
    product_id: str
    product_name: str
    category: str
    similarity: float
    discounted_price: float
    rating: float
    rating_count: int
    review_analysis: ReviewAnalysis

class PredictionInput(BaseModel):
    price: float
    review_count: int
    category: str
    review_count_distribution: List[Dict[str, Any]]
    rating_distribution: List[Dict[str, Any]]

class PriceRangeResponse(BaseModel):
    min_price: float
    max_price: float

class DistributionBin(BaseModel):
    name: str
    count: int

class CategoryStatsResponse(BaseModel):
    min_price: float
    max_price: float
    min_review_count: float
    max_review_count: float
    price_distribution: List[DistributionBin]
    review_count_distribution: List[DistributionBin]
    rating_distribution: List[DistributionBin]

# --- FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ---
app = FastAPI()

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€: Next.js ì•±(http://localhost:3000)ì—ì„œì˜ ìš”ì²­ì„ í—ˆìš©í•©ë‹ˆë‹¤.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ì „ì—­ ë³€ìˆ˜: ëª¨ë¸, ë°ì´í„°, ì „ì²˜ë¦¬ê¸° ---
ml_pipe = None
review_count_pipe = None # ë¦¬ë·° ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸
hierarchical_categories_data = {} # ê³„ì¸µì  ì¹´í…Œê³ ë¦¬ ë°ì´í„°
tfidf_vectorizer = None
tfidf_matrix = None
df_products: pd.DataFrame = pd.DataFrame() # ìƒí’ˆ ë©”íƒ€ë°ì´í„° ë° ìœ ì‚¬ë„ ë¶„ì„ìš©
df_reviews: pd.DataFrame = pd.DataFrame() # ìƒí’ˆë³„ ê°œë³„ ë¦¬ë·° ì €ì¥ìš©
DATA_FILE_PATH = os.path.join("data", "cleaned_amazon_0519.csv")

# --- í•µì‹¬ ë¡œì§: ë°ì´í„° ë¡œë”© ë° ëª¨ë¸ í•™ìŠµ ---
def load_data_and_train_models():
    global ml_pipe, review_count_pipe, tfidf_vectorizer, tfidf_matrix, df_products, df_reviews, hierarchical_categories_data
    
    if not os.path.exists(DATA_FILE_PATH):
        print(f"âš ï¸ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DATA_FILE_PATH}")
        df_products, df_reviews = pd.DataFrame(), pd.DataFrame()
        return

    df_raw = pd.read_csv(DATA_FILE_PATH)
    
    required_columns = ['product_id', 'product_name', 'about_product', 'review_title', 'review_content', 'discounted_price', 'rating_count', 'category_cleaned', 'rating']
    if any(col not in df_raw.columns for col in required_columns):
        missing_cols = [col for col in required_columns if col not in df_raw.columns]
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ì¤‘ ì¼ë¶€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_cols)}")

    # --- 1. ê¸°ë³¸ í´ë¦¬ë‹ ë° íƒ€ì… ë³€í™˜ ---
    df_raw['about_product'] = df_raw['about_product'].fillna('')
    df_raw['review_title'] = df_raw['review_title'].fillna('')
    df_raw['review_content'] = df_raw['review_content'].fillna('')
    for col in ['discounted_price', 'rating_count', 'rating']:
        df_raw[col] = pd.to_numeric(df_raw[col], errors='coerce')
    df_raw.dropna(subset=['product_id', 'discounted_price', 'rating_count', 'rating', 'category_cleaned'], inplace=True)

    # --- 2. ë¦¬ë·° ë¶„ë¦¬ ë° df_reviews ìƒì„± (ì œëª© + ë‚´ìš© ê²°í•©) ---
    reviews_list = []
    for _, row in df_raw.iterrows():
        # review_contentë¥¼ ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ ê°œë³„ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ ìƒì„±
        contents = [c.strip() for c in str(row['review_content']).split(',') if c.strip()]
        
        # ë¦¬ë·° ì œëª©ì„ ë‚´ìš© ì•ì— ë¶™ì—¬ì¤Œ
        title = str(row['review_title']).strip()
        
        for content_part in contents:
            # ì œëª©ê³¼ ë‚´ìš©ì„ í•©ì³ì„œ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¦
            full_review_text = (title + ' ' + content_part).strip()
            reviews_list.append({
                'product_id': row['product_id'],
                'review_text': full_review_text
            })
    
    df_reviews = pd.DataFrame(reviews_list)
    
    if df_reviews.empty:
        print("âš ï¸ ë¦¬ë·° ë°ì´í„°ë¥¼ ë¶„ë¦¬í•œ í›„ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        df_products = pd.DataFrame()
        return

    # --- 3. ìœ ì‚¬ë„ ë¶„ì„ìš© df_products ìƒì„± ---
    # ìƒí’ˆ ë©”íƒ€ë°ì´í„°(ë¦¬ë·° ì œì™¸)ë¥¼ ê°€ì ¸ì™€ ì¤‘ë³µ ì œê±°
    # 'about_product'ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
    df_meta_cols = ['product_id', 'product_name', 'about_product', 'category_cleaned', 'discounted_price', 'rating_count', 'rating']
    df_products = df_raw[df_meta_cols].drop_duplicates(subset=['product_id']).copy() # type: ignore
    
    # ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëª¨ë‘ ìˆëŠ”ì§€ ìµœì¢… í™•ì¸
    df_products.dropna(subset=['discounted_price', 'rating_count', 'rating', 'category_cleaned'], inplace=True)
    
    if df_products.empty:
        print("âš ï¸ ìµœì¢… ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
        
    # --- 4. ëª¨ë¸ í•™ìŠµ ---
    numeric_features = ['discounted_price']
    categorical_features = ['category_cleaned']
    
    # ì…ë ¥ íŠ¹ì„±ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
    X_features = df_products[numeric_features + categorical_features]

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)])

    # ëª¨ë¸ 1: ë³„ì  ì˜ˆì¸¡(y_rating)
    y_rating = df_products['rating']
    ml_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', Ridge(alpha=1.0))])
    ml_pipe.fit(X_features, y_rating)
    print("âœ… Rating Prediction model training complete!")

    # ëª¨ë¸ 2: ë¦¬ë·° ìˆ˜ ì˜ˆì¸¡(y_review_count)
    y_review_count = df_products['rating_count']
    review_count_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', Ridge(alpha=1.0))])
    review_count_pipe.fit(X_features, y_review_count)
    print("âœ… Review Count Prediction model training complete!")

    # TF-IDF ëª¨ë¸ í•™ìŠµ (ìœ ì‚¬ë„ ë¶„ì„ìš©) - 'about_product' ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
    tfidf_matrix = tfidf_vectorizer.fit_transform(df_products['about_product']) # type: ignore
    print("âœ… TF-IDF model (based on product description) training complete!")
    
    # --- 5. ê³„ì¸µì  ì¹´í…Œê³ ë¦¬ ë°ì´í„° ìƒì„± ---
    temp_hierarchical_data = {}
    for cat_string in df_products['category_cleaned'].unique():
        parts = cat_string.split(' | ')
        current_level = temp_hierarchical_data
        for part in parts:
            if part not in current_level:
                current_level[part] = {}
            current_level = current_level[part]
    hierarchical_categories_data = temp_hierarchical_data
    print("âœ… Hierarchical category data created!")

    print(f"ğŸ“ˆ Total {len(df_products)} unique products and {len(df_reviews)} individual reviews loaded.")
    print(f"â­ Rating range found in data: {df_products['rating'].min()} ~ {df_products['rating'].max()}")


# --- ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰ë  ë¡œì§ ---
@app.on_event("startup")
def startup_event():
    try:
        load_data_and_train_models()
    except Exception as e:
        print(f"ğŸš¨ ì„œë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# --- API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---
@app.get("/")
def read_root():
    return {"message": "Rmazon predictor and similarity API is running!"}

@app.post("/upload-csv")
async def upload_csv(file: UploadFile = File(...)):
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_file_path = f"temp_{file.filename}"
    with open(temp_file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    # ìƒˆ íŒŒì¼ ê²€ì¦
    try:
        df_new = pd.read_csv(temp_file_path)
        required_columns = ['product_id', 'product_name', 'about_product', 'review_title', 'review_content', 'discounted_price', 'rating_count', 'category_cleaned', 'rating']
        missing = [col for col in required_columns if col not in df_new.columns]
        if missing:
            raise HTTPException(status_code=400, detail=f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {', '.join(missing)}")
        
        # ê²€ì¦ í†µê³¼ ì‹œ, ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°
        shutil.move(temp_file_path, DATA_FILE_PATH)
        
        # ë°ì´í„°ì™€ ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œ
        load_data_and_train_models()
        
        return {"message": "íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œ ë° ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.", "rows": len(df_new)}
    
    except Exception as e:
        os.remove(temp_file_path) # ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ ì œê±°
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        await file.close()

@app.get("/categories", response_model=List[str])
def get_categories():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df_products.empty:
        return []
    return sorted(df_products['category_cleaned'].unique().tolist())

@app.get("/hierarchical-categories", response_model=Dict)
def get_hierarchical_categories():
    """ê³„ì¸µ êµ¬ì¡°ì˜ ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not hierarchical_categories_data:
        raise HTTPException(status_code=503, detail="ì„œë²„ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return hierarchical_categories_data

@app.get("/product-count", response_model=int)
def get_product_count(category: str = Query(..., description="ìƒí’ˆ ìˆ˜ë¥¼ ì¡°íšŒí•  ì „ì²´ ì¹´í…Œê³ ë¦¬ ê²½ë¡œ")):
    """ì„ íƒëœ ì „ì²´ ì¹´í…Œê³ ë¦¬ ê²½ë¡œì— í•´ë‹¹í•˜ëŠ” ìƒí’ˆì˜ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df_products.empty:
        raise HTTPException(status_code=503, detail="ì„œë²„ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    count = df_products[df_products['category_cleaned'] == category].shape[0]
    return count

@app.get("/products", response_model=List[Product])
def get_products(category: Optional[str] = Query(None)):
    """
    ìƒí’ˆ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - category ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ìƒí’ˆë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    if df_products.empty:
        return []
    
    if category:
        filtered_df = df_products[df_products['category_cleaned'] == category]
        return filtered_df[['product_id', 'product_name']].to_dict('records') # type: ignore
    
    # ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ ëª©ë¡ ë°˜í™˜ (ê´€ë¦¬ìš© ë˜ëŠ” ë‹¤ë¥¸ ìš©ë„ë¡œ ìœ ì§€)
    return df_products[['product_id', 'product_name']].to_dict('records') # type: ignore

# --- ê³ ê¸‰ ë¦¬ë·° ë¶„ì„ ë¡œì§ (ì„œë²„ ì‚¬ì´ë“œë¡œ ì´ë™) ---
def advanced_review_analysis(reviews: List[str]) -> Dict[str, Any]:
    if not reviews or all(r is None for r in reviews):
        return {
            "positive_percentage": 0, "negative_percentage": 0, "neutral_percentage": 100,
            "top_positive_keywords": [], "top_negative_keywords": []
        }

    valid_reviews = [r for r in reviews if r is not None]
    if not valid_reviews:
        return {
            "positive_percentage": 0, "negative_percentage": 0, "neutral_percentage": 100,
            "top_positive_keywords": [], "top_negative_keywords": []
        }
        
    # ngram_range=(1, 2)ë¡œ ìˆ˜ì •: 1ê°œ ë‹¨ì–´ ë° 2ê°œ ë‹¨ì–´ ì¡°í•© ëª¨ë‘ ë¶„ì„
    vectorizer = TfidfVectorizer(stop_words='english', max_features=100, ngram_range=(1, 2))
    tfidf_matrix_local = vectorizer.fit_transform(valid_reviews)
    
    sid = SentimentIntensityAnalyzer()
    
    # 1. ê° ë¦¬ë·°ì˜ ì „ì²´ ê°ì„± ì ìˆ˜(compound)ë¥¼ ë¯¸ë¦¬ ê³„ì‚°
    review_sentiments = [sid.polarity_scores(review)['compound'] for review in valid_reviews]
    
    # 2. ê° í‚¤ì›Œë“œê°€ ì–´ë–¤ ë¦¬ë·°ë“¤ì—ì„œ ë“±ì¥í–ˆëŠ”ì§€, í•´ë‹¹ ë¦¬ë·°ì˜ ì ìˆ˜ì™€ í•¨ê»˜ ê¸°ë¡
    feature_names = vectorizer.get_feature_names_out()
    keyword_sentiments: Dict[str, List[float]] = {kw: [] for kw in feature_names}
    
    # í¬ì†Œ í–‰ë ¬(tfidf_matrix_local)ì„ ìˆœíšŒí•˜ë©° í‚¤ì›Œë“œì™€ ë¦¬ë·° ì ìˆ˜ë¥¼ ì—°ê²°
    cx = tfidf_matrix_local.tocoo() # type: ignore
    for review_idx, keyword_idx in zip(cx.row, cx.col):
        keyword = feature_names[keyword_idx]
        compound_score = review_sentiments[review_idx]
        keyword_sentiments[keyword].append(compound_score)

    # 3. ê° í‚¤ì›Œë“œì˜ í‰ê·  ê°ì„± ì ìˆ˜ë¥¼ ê³„ì‚°
    avg_keyword_sentiments: Dict[str, float] = {}
    for kw, scores in keyword_sentiments.items():
        if scores:
            avg_keyword_sentiments[kw] = sum(scores) / len(scores)

    # 4. í‰ê·  ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì •ë ¬ (ë¶€ì •->ê¸ì • ìˆœ)
    sorted_keywords = sorted(avg_keyword_sentiments.items(), key=lambda item: item[1])

    # 5. ê¸ˆì§€ì–´ ëª©ë¡ì„ ì‚¬ìš©í•œ í•„í„°ë§
    FORCE_NEGATIVE_WORDS = {"disappoint", "bad", "poor", "broken", "issue", "problem", "fail", "slow", "garbage", "useless", "worst", "not working", "stopped"}
    FORCE_POSITIVE_WORDS = {"good", "great", "excellent", "love", "amazing", "perfect", "best", "nice", "works", "fast", "easy"}

    def is_positive_candidate(kw: str) -> bool:
        # ë¶€ì • ê¸ˆì§€ì–´ê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ê¸ì • í›„ë³´ì—ì„œ íƒˆë½
        return not any(neg_word in kw for neg_word in FORCE_NEGATIVE_WORDS)

    def is_negative_candidate(kw: str) -> bool:
        # ê¸ì • ê¸ˆì§€ì–´ê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ë¶€ì • í›„ë³´ì—ì„œ íƒˆë½
        return not any(pos_word in kw for pos_word in FORCE_POSITIVE_WORDS)

    # 6. í•„í„°ë§ì„ í†µí•´ ì§„ì§œ ê¸ì •/ë¶€ì • í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
    # ë¶€ì • í‚¤ì›Œë“œ: í‰ê·  ì ìˆ˜ê°€ 0 ë¯¸ë§Œì´ê³ , ê¸ì • ê¸ˆì§€ì–´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œ
    negative_candidates = [
        (kw, score) for kw, score in sorted_keywords 
        if score < 0 and is_negative_candidate(kw)
    ]
    top_negative_keywords = [kw for kw, score in negative_candidates[:5]]

    # ê¸ì • í‚¤ì›Œë“œ: í‰ê·  ì ìˆ˜ê°€ 0 ì´ˆê³¼ì´ê³ , ë¶€ì • ê¸ˆì§€ì–´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œ
    positive_candidates = [
        (kw, score) for kw, score in sorted_keywords 
        if score > 0 and is_positive_candidate(kw)
    ]
    # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ë‹¤ì‹œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œë¥¼ ì„ íƒ
    top_positive_keywords = [kw for kw, score in sorted(positive_candidates, key=lambda item: item[1], reverse=True)[:5]]

    # --- ì „ì²´ ê¸/ë¶€ì • ë¹„ìœ¨ ê³„ì‚° ë¡œì§ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
    pos_count, neg_count, neu_count = 0, 0, 0
    for score in review_sentiments:
        if score >= 0.05:
            pos_count += 1
        elif score <= -0.05:
            neg_count += 1
        else:
            neu_count += 1
            
    total_reviews = len(valid_reviews)
    positive_percentage = (pos_count / total_reviews) * 100 if total_reviews > 0 else 0
    negative_percentage = (neg_count / total_reviews) * 100 if total_reviews > 0 else 0
    neutral_percentage = (neu_count / total_reviews) * 100 if total_reviews > 0 else 100

    return {
        "positive_percentage": positive_percentage,
        "negative_percentage": negative_percentage,
        "neutral_percentage": neutral_percentage,
        "top_positive_keywords": top_positive_keywords,
        "top_negative_keywords": top_negative_keywords
    }

@app.get("/category-price-range", response_model=PriceRangeResponse)
def get_category_price_range(category: str = Query(...)):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ìµœì†Œ ë° ìµœëŒ€ ê°€ê²©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df_products.empty:
        raise HTTPException(status_code=503, detail="ì„œë²„ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    filtered_df = df_products[df_products['category_cleaned'] == category]
    
    if filtered_df.empty:
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ìƒí’ˆì´ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„°ì˜ ê°€ê²© ë²”ìœ„ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì œê³µ
        min_price = df_products['discounted_price'].min()
        max_price = df_products['discounted_price'].max()
    else:
        min_price = filtered_df['discounted_price'].min()
        max_price = filtered_df['discounted_price'].max()

    return {"min_price": min_price, "max_price": max_price}

@app.get("/category-stats", response_model=CategoryStatsResponse)
def get_category_stats(category: str = Query(..., description="í†µê³„ ì •ë³´ë¥¼ ì¡°íšŒí•  ì¹´í…Œê³ ë¦¬")):
    """ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ì˜ ê°€ê²©, ë¦¬ë·° ìˆ˜, ë³„ì ì— ëŒ€í•œ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df_products.empty:
        raise HTTPException(status_code=503, detail="ì„œë²„ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    filtered_df = df_products[df_products['category_cleaned'] == category]
    if filtered_df.empty:
        raise HTTPException(status_code=404, detail=f"'{category}' ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def create_histogram(data: pd.Series, bins=10) -> List[Dict[str, Any]]:
        if data.empty:
            return []
        # NaN ê°’ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ê³ , ë°ì´í„° íƒ€ì…ì„ floatìœ¼ë¡œ í†µì¼
        data = pd.to_numeric(data, errors='coerce').dropna() # type: ignore
        if data.empty:
            return []
        
        min_val, max_val = data.min(), data.max()
        
        if min_val == max_val:
            return [{"name": f"{min_val:.0f}-{max_val:.0f}", "count": len(data)}]

        try:
            # np.histogramì„ ì‚¬ìš©í•˜ì—¬ ë¹ˆê³¼ ì¹´ìš´íŠ¸ ê³„ì‚°
            counts, bin_edges = np.histogram(data, bins=bins, range=(min_val, max_val))
        except Exception:
            # ì˜ˆì™¸ ë°œìƒ ì‹œ ìˆ˜ë™ìœ¼ë¡œ ë²”ìœ„ ê³„ì‚°
            bin_edges = np.linspace(min_val, max_val, bins + 1)
            counts = pd.cut(data, bins=bin_edges, include_lowest=True, right=False).value_counts().sort_index().values # type: ignore

        
        histogram = []
        for i in range(len(counts)):
            # ë ˆì´ë¸” í˜•ì‹ ë³€ê²½ (ì •ìˆ˜í˜•ìœ¼ë¡œ)
            start = int(bin_edges[i])
            end = int(bin_edges[i+1])
            histogram.append({"name": f"{start:,}-{end:,}", "count": int(counts[i])})
        
        # ë§ˆì§€ë§‰ ë¹ˆì— ìµœëŒ€ê°’ í¬í•¨ì‹œí‚¤ê¸°
        if len(histogram) > 0 and max_val == bin_edges[-1]:
            last_item_name = histogram[-1]['name']
            if str(max_val) not in last_item_name.split('-')[1]:
                # ë°ì´í„°ê°€ ë§ˆì§€ë§‰ ë¹ˆì˜ ê²½ê³„ì— ì •í™•íˆ ìˆì„ ë•Œ ì¹´ìš´íŠ¸ë¥¼ ë§ˆì§€ë§‰ ë¹ˆì— í¬í•¨
                final_count = data[data >= bin_edges[-2]].count() # type: ignore
                histogram[-1]['count'] = int(final_count)


        return histogram

    return {
        "min_price": filtered_df['discounted_price'].min(),
        "max_price": filtered_df['discounted_price'].max(),
        "min_review_count": filtered_df['rating_count'].min(),
        "max_review_count": filtered_df['rating_count'].max(),
        "price_distribution": create_histogram(filtered_df['discounted_price']), # type: ignore
        "review_count_distribution": create_histogram(filtered_df['rating_count']), # type: ignore
        "rating_distribution": create_histogram(filtered_df['rating'], bins=8) # type: ignore
    }

@app.post("/search-similarity", response_model=List[SimilarityResult])
def search_similarity(request: SimilarityRequest):
    """
    ì…ë ¥ëœ ìƒí’ˆ ì •ë³´ì™€ ê°€ì¥ ìœ ì‚¬í•œ ìƒí’ˆ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìœ ì‚¬ë„ëŠ” í…ìŠ¤íŠ¸(TF-IDF)ì™€ ê°€ê²©ì„ ì¢…í•©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤.
    ê° ìœ ì‚¬ ìƒí’ˆì— ëŒ€í•´ ê°œë³„ì ì¸ ë¦¬ë·° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if df_products.empty or tfidf_matrix is None or tfidf_vectorizer is None:
        raise HTTPException(status_code=503, detail="ì„œë²„ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    if not request.description.strip() or not request.category:
        raise HTTPException(status_code=400, detail="ìƒí’ˆ ì„¤ëª…ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (TF-IDF)
    input_vector = tfidf_vectorizer.transform([request.description])
    text_similarities = cosine_similarity(input_vector, tfidf_matrix).flatten()

    # 2. ì¹´í…Œê³ ë¦¬ê°€ ì¼ì¹˜í•˜ëŠ” ìƒí’ˆë§Œ í•„í„°ë§
    category_mask = df_products['category_cleaned'] == request.category
    
    # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
    # ê°€ê²© ìœ ì‚¬ë„: ìš”ì²­ ê°€ê²©ê³¼ì˜ ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ë†’ìŒ (ì •ê·œí™”)
    price_diff = np.abs(df_products['discounted_price'] - request.price)
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì•„ì£¼ ì‘ì€ ê°’(epsilon)ì„ ë”í•¨
    price_similarity = 1 - (price_diff / (price_diff.max() + 1e-6))
    
    # ì¢…í•© ì ìˆ˜ = í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ * 0.7 + ê°€ê²© ìœ ì‚¬ë„ * 0.3
    combined_scores = (text_similarities * 0.7) + (price_similarity * 0.3)
    
    # ì¹´í…Œê³ ë¦¬ ë§ˆìŠ¤í¬ ì ìš©
    combined_scores[~category_mask] = 0

    # 4. ìƒìœ„ 5ê°œ ìƒí’ˆ ì„ ì •
    top_n = 5
    # ì ìˆ˜ê°€ 0ë³´ë‹¤ í° ìƒí’ˆë“¤ë§Œ í•„í„°ë§
    valid_scores = combined_scores[combined_scores > 0]
    
    if valid_scores.empty:
        return []

    # ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ Nê°œì˜ ìƒí’ˆì„ ì„ íƒ (ê²°ê³¼ëŠ” 'ì¸ë±ìŠ¤ ë¼ë²¨: ì ìˆ˜' í˜•íƒœì˜ Series)
    top_products = valid_scores.nlargest(top_n)

    # 5. ìµœì¢… ê²°ê³¼ ìƒì„± (ìƒí’ˆë³„ ê°œë³„ ë¶„ì„)
    results = []
    # top_products.items()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ë±ìŠ¤ ë¼ë²¨(idx_label)ê³¼ ì ìˆ˜(score)ë¥¼ í•¨ê»˜ ìˆœíšŒ
    for idx_label, score in top_products.items():
        # .locë¥¼ ì‚¬ìš©í•˜ì—¬ ë¼ë²¨ ê¸°ë°˜ìœ¼ë¡œ ìƒí’ˆ ì •ë³´ ì¡°íšŒ
        product_row = df_products.loc[idx_label]
        product_id = product_row['product_id']
        
        # ìƒí’ˆë³„ ê°œë³„ ë¦¬ë·° ì¶”ì¶œ
        product_reviews = df_reviews[df_reviews['product_id'] == product_id]['review_text'].tolist()
        
        # ìƒí’ˆë³„ ë¦¬ë·° ë¶„ì„ ìˆ˜í–‰
        if not product_reviews:
            # ë¦¬ë·°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            review_analysis_result = {
                'positive_percentage': 0, 'negative_percentage': 0, 'neutral_percentage': 100,
                'top_positive_keywords': [], 'top_negative_keywords': []
            }
        else:
            review_analysis_result = advanced_review_analysis(product_reviews)
            
        results.append(SimilarityResult(
            product_id=product_id,
            product_name=product_row['product_name'],
            category=product_row['category_cleaned'],
            similarity=score,  # nlargestì—ì„œ ì–»ì€ ì •í™•í•œ ì ìˆ˜ ì‚¬ìš©
            discounted_price=product_row['discounted_price'],
            rating=product_row['rating'],
            rating_count=product_row['rating_count'],
            review_analysis=ReviewAnalysis(**review_analysis_result)
        ))
        
    return results

@app.post("/predict", response_model=PredictionResponse)
def predict_star_rating(request: PredictionRequest):
    if ml_pipe is None or review_count_pipe is None or df_products.empty:
        raise HTTPException(status_code=503, detail="ì˜ˆì¸¡ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    input_data_dict = {
        'discounted_price': request.price,
        'category_cleaned': request.category
    }
    input_data = pd.DataFrame([input_data_dict])
    
    # ë³„ì  ë° ë¦¬ë·° ìˆ˜ ì˜ˆì¸¡
    predicted_star = ml_pipe.predict(input_data)[0]
    predicted_review_count = review_count_pipe.predict(input_data)[0]
    
    # ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ í˜„ì‹¤ì ì¸ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ë³´ì •
    clamped_star = max(0.0, min(5.0, predicted_star))
    clamped_review_count = max(0.0, predicted_review_count) # ë¦¬ë·° ìˆ˜ëŠ” ìŒìˆ˜ê°€ ë  ìˆ˜ ì—†ìŒ

    # --- ë°±ë¶„ìœ„ ê³„ì‚° ë¡œì§ ---
    filtered_df = df_products[df_products['category_cleaned'] == request.category]
    
    def calculate_percentile(series: pd.Series, score: float) -> float:
        if series.empty: return 50.0  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’ìœ¼ë¡œ ì²˜ë¦¬
        return (series < score).sum() / len(series) * 100

    price_percentile = calculate_percentile(filtered_df['discounted_price'], request.price) # type: ignore
    review_count_percentile = calculate_percentile(filtered_df['rating_count'], clamped_review_count) # type: ignore
    rating_percentile = calculate_percentile(filtered_df['rating'], clamped_star) # type: ignore
    
    return {
        "predicted_star": round(clamped_star, 2),
        "predicted_review_count": round(clamped_review_count, 0),
        "price_percentile": round(price_percentile, 1),
        "review_count_percentile": round(review_count_percentile, 1),
        "rating_percentile": round(rating_percentile, 1),
    } 