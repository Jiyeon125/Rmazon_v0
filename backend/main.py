from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
from pandas import DataFrame
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import text
import os
import shutil
from typing import List, Optional, Dict, Any
import numpy as np
import joblib
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from scipy.sparse import csr_matrix
from collections import Counter

# --- Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÌÇ§ÏõåÎìú ÏÑ∏Ìä∏ ---
# Ïù¥Ï†ÑÏóê ÏÇ¨Ïö©ÎêòÎçò Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÌÇ§ÏõåÎìúÏÖãÏùÄ TECH_KEYWORDS Î∞©ÏãùÏúºÎ°ú ÎåÄÏ≤¥ÎêòÏóàÏäµÎãàÎã§.
KEYWORD_SETS = {}

# Ï†úÌíàÏùò ÌïµÏã¨ Í∏∞Îä•ÏùÑ Ï†ïÏùòÌïòÎäî ÌÇ§ÏõåÎìúÏÖã
# Ïù¥ ÌÇ§ÏõåÎìúÎì§ÏùÄ Ï†úÌíàÏùò Ï¢ÖÎ•òÎ•º Î™ÖÌôïÌûà Íµ¨Î∂ÑÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê®
TECH_KEYWORDS = [
    'usb-c', 'usb-a', 'hdmi', '8-pin', 'lightning', 'micro-usb', 'type-c',
    'bluetooth', 'wireless', 'wired', '5g', '4g', 'lte', 'wifi',
    'ssd', 'hdd', 'ddr4', 'ddr5', 'ram', 'gb', 'tb',
    'led', 'lcd', 'oled', 'qled', '4k', '8k', '1080p',
    'noise cancelling', 'waterproof', 'water resistant',
    'sata', 'nvme', 'm.2'
]

# --- Pydantic Î™®Îç∏ Ï†ïÏùò ---
# API ÏöîÏ≤≠/ÏùëÎãµÏùò Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Î•º Ï†ïÏùòÌïòÏó¨ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨ Î∞è Î¨∏ÏÑúÌôîÎ•º ÏûêÎèôÌôîÌï©ÎãàÎã§.
class Keyword(BaseModel):
    """Î¶¨Î∑∞ Î∂ÑÏÑù Í≤∞Í≥ºÏóêÏÑú ÏÇ¨Ïö©Îê† ÌÇ§ÏõåÎìúÏôÄ ÎπàÎèÑÏàò Î™®Îç∏"""
    word: str
    count: int

class PredictionRequest(BaseModel):
    """ÌåêÎß§ ÏßÄÌëú ÏòàÏ∏° ÏöîÏ≤≠ Î™®Îç∏"""
    price: float
    category: str

class PredictionResponse(BaseModel):
    """ÌåêÎß§ ÏßÄÌëú ÏòàÏ∏° Í≤∞Í≥º ÏùëÎãµ Î™®Îç∏"""
    predicted_star: float
    predicted_review_count: float
    price_percentile: float
    review_count_percentile: float
    rating_percentile: float

class SimilarityRequest(BaseModel):
    """Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ ÏöîÏ≤≠ Î™®Îç∏"""
    description: str
    price: float
    discount_percentage: float
    category: str

class Product(BaseModel):
    """Í∏∞Î≥∏ ÏÉÅÌíà Ï†ïÎ≥¥ Î™®Îç∏"""
    product_id: str
    product_name: str

class ReviewAnalysis(BaseModel):
    """Î¶¨Î∑∞ Î∂ÑÏÑù Í≤∞Í≥º Î™®Îç∏"""
    positive_percentage: float
    negative_percentage: float
    neutral_percentage: float
    top_positive_keywords: List[str]
    top_negative_keywords: List[str]

class ProductInfo(BaseModel):
    """ÏÉÅÌíàÏùò Ï¢ÖÌï© Ï†ïÎ≥¥ Î™®Îç∏"""
    product_id: str
    product_name: str
    category: str
    price: float
    review_count: int
    review_analysis: ReviewAnalysis

class SimilarProduct(ProductInfo):
    """Ïú†ÏÇ¨ÎèÑ Ï†êÏàòÍ∞Ä Ìè¨Ìï®Îêú ÏÉÅÌíà Ï†ïÎ≥¥ Î™®Îç∏ (ÌòÑÏû¨Îäî ÏßÅÏ†ë ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÏùå)"""
    similarity: float

class SimilarityResult(BaseModel):
    """Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ Í≤∞Í≥ºÎ°ú Î∞òÌôòÎê† Í∞Å ÏÉÅÌíàÏùò ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Î™®Îç∏"""
    product_id: str
    product_name: str
    category: str
    similarity: float # ÏµúÏ¢Ö Ïú†ÏÇ¨ÎèÑ Ï†êÏàò (TF-IDF + ÌÇ§ÏõåÎìú Ï†êÏàò)
    discounted_price: float
    rating: float
    rating_count: int
    review_analysis: ReviewAnalysis

class PriceRangeResponse(BaseModel):
    """Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Í∞ÄÍ≤© Î≤îÏúÑ ÏùëÎãµ Î™®Îç∏"""
    min_price: float
    max_price: float

class DistributionBin(BaseModel):
    """Î∂ÑÌè¨ÎèÑ Ï∞®Ìä∏Ïùò Í∞Å ÎßâÎåÄÎ•º ÎÇòÌÉÄÎÇ¥Îäî Î™®Îç∏"""
    name: str
    count: int

class CategoryStatsResponse(BaseModel):
    """Ïπ¥ÌÖåÍ≥†Î¶¨ ÌÜµÍ≥Ñ Ï†ïÎ≥¥ ÏùëÎãµ Î™®Îç∏"""
    min_price: float
    max_price: float
    min_review_count: float
    max_review_count: float
    price_distribution: List[DistributionBin]
    review_count_distribution: List[DistributionBin]
    rating_distribution: List[DistributionBin]

# --- FastAPI Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏÑ§Ï†ï ---
app = FastAPI()

# CORS ÎØ∏Îì§Ïõ®Ïñ¥ Ï∂îÍ∞Ä: Next.js Ïï±(http://localhost:3000)ÏóêÏÑúÏùò ÏöîÏ≤≠ÏùÑ ÌóàÏö©Ìï©ÎãàÎã§.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Ï†ÑÏó≠ Î≥ÄÏàò: Î™®Îç∏, Îç∞Ïù¥ÌÑ∞, Ï†ÑÏ≤òÎ¶¨Í∏∞ ---
ml_pipe: Optional[Pipeline] = None
review_count_pipe: Optional[Pipeline] = None # Î¶¨Î∑∞ Ïàò ÏòàÏ∏° Î™®Îç∏
hierarchical_categories_data: Dict = {} # Í≥ÑÏ∏µÏ†Å Ïπ¥ÌÖåÍ≥†Î¶¨ Îç∞Ïù¥ÌÑ∞
tfidf_vectorizer: Optional[TfidfVectorizer] = None
tfidf_matrix: Optional[csr_matrix] = None
df_products: pd.DataFrame = pd.DataFrame() # ÏÉÅÌíà Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∞è Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑùÏö©
df_reviews: pd.DataFrame = pd.DataFrame() # ÏÉÅÌíàÎ≥Ñ Í∞úÎ≥Ñ Î¶¨Î∑∞ Ï†ÄÏû•Ïö©
DATA_FILE_PATH = os.path.join("data", "cleaned_amazon_0519.csv")

# --- ÌïµÏã¨ Î°úÏßÅ: Îç∞Ïù¥ÌÑ∞ Î°úÎî© Î∞è Î™®Îç∏ ÌïôÏäµ ---
def load_data_and_train_models():
    """
    ÏÑúÎ≤Ñ ÏãúÏûë Ïãú Ïã§ÌñâÎêòÎäî ÌïµÏã¨ Ìï®Ïàò.
    1. CSV Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ ÏùΩÏñ¥Îì§ÏûÖÎãàÎã§.
    2. Îç∞Ïù¥ÌÑ∞Î•º Ï†ïÏ†úÌïòÍ≥† Í∏∞Î≥∏ ÌÉÄÏûÖÏùÑ Î≥ÄÌôòÌï©ÎãàÎã§.
    3. 'review_content'Ïóê ÏâºÌëúÎ°ú Ìï©Ï≥êÏßÑ Î¶¨Î∑∞Îì§ÏùÑ Î∂ÑÎ¶¨ÌïòÏó¨ ÏÉÅÌíàÎ≥Ñ Í∞úÎ≥Ñ Î¶¨Î∑∞(df_reviews)Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
       Ïù¥ Í≥ºÏ†ïÏóêÏÑú Îçî Ï†ïÌôïÌïú Î∂ÑÏÑùÏùÑ ÏúÑÌï¥ Î¶¨Î∑∞ Ï†úÎ™©('review_title')Í≥º ÎÇ¥Ïö©('review_content')ÏùÑ Í≤∞Ìï©Ìï©ÎãàÎã§.
    4. ÏÉÅÌíà Î©îÌÉÄÎç∞Ïù¥ÌÑ∞(df_products)Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
    5. df_productsÎ•º Í∏∞Î∞òÏúºÎ°ú 3Í∞ÄÏßÄ Î®∏Ïã†Îü¨Îãù Î™®Îç∏ÏùÑ ÌïôÏäµÌï©ÎãàÎã§:
        - Î≥ÑÏ†ê ÏòàÏ∏° Î™®Îç∏ (ml_pipe)
        - Î¶¨Î∑∞ Ïàò ÏòàÏ∏° Î™®Îç∏ (review_count_pipe)
        - ÏÉÅÌíà ÏÑ§Î™Ö Í∏∞Î∞ò TF-IDF Î™®Îç∏ (tfidf_vectorizer, tfidf_matrix)
    6. ÌîÑÎ°†Ìä∏ÏóîÎìúÏóêÏÑú ÏÇ¨Ïö©Ìï† Í≥ÑÏ∏µÏ†Å Ïπ¥ÌÖåÍ≥†Î¶¨ Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
    """
    global ml_pipe, review_count_pipe, tfidf_vectorizer, tfidf_matrix, df_products, df_reviews, hierarchical_categories_data
    
    if not os.path.exists(DATA_FILE_PATH):
        print(f"‚ö†Ô∏è Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§: {DATA_FILE_PATH}")
        df_products, df_reviews = pd.DataFrame(), pd.DataFrame()
        return

    df_raw = pd.read_csv(DATA_FILE_PATH)
    
    required_columns = ['product_id', 'product_name', 'about_product', 'review_title', 'review_content', 'discounted_price', 'rating_count', 'category_cleaned', 'rating']
    if any(col not in df_raw.columns for col in required_columns):
        missing_cols = [col for col in required_columns if col not in df_raw.columns]
        raise ValueError(f"ÌïÑÏàò Ïª¨Îüº Ï§ë ÏùºÎ∂ÄÍ∞Ä ÎàÑÎùΩÎêòÏóàÏäµÎãàÎã§: {', '.join(missing_cols)}")

    # --- 1. Í∏∞Î≥∏ ÌÅ¥Î¶¨Îãù Î∞è ÌÉÄÏûÖ Î≥ÄÌôò ---
    df_raw['about_product'] = df_raw['about_product'].fillna('')
    df_raw['review_title'] = df_raw['review_title'].fillna('')
    df_raw['review_content'] = df_raw['review_content'].fillna('')
    for col in ['discounted_price', 'rating_count', 'rating']:
        df_raw[col] = pd.to_numeric(df_raw[col], errors='coerce')
    df_raw.dropna(subset=['product_id', 'discounted_price', 'rating_count', 'rating', 'category_cleaned'], inplace=True)

    # --- 2. Î¶¨Î∑∞ Î∂ÑÎ¶¨ Î∞è df_reviews ÏÉùÏÑ± (Ï†úÎ™© + ÎÇ¥Ïö© Í≤∞Ìï©) ---
    # CSVÏùò Ìïú ÏÖÄÏóê Î™®Îì† Î¶¨Î∑∞Í∞Ä ÏâºÌëúÎ°ú Ìï©Ï≥êÏ†∏ ÏûàÎäî Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥,
    # Í∞Å ÏÉÅÌíàÏùò Î™®Îì† Î¶¨Î∑∞Î•º Í∞úÎ≥Ñ ÌñâÏúºÎ°ú Î∂ÑÎ¶¨ÌïòÏó¨ Î≥ÑÎèÑÏùò DataFrame(df_reviews)ÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§.
    reviews_list = []
    for _, row in df_raw.iterrows():
        contents = [c.strip() for c in str(row['review_content']).split(',') if c.strip()]
        title = str(row['review_title']).strip()
        for content_part in contents:
            full_review_text = (title + ' ' + content_part).strip()
            reviews_list.append({'product_id': row['product_id'], 'review_text': full_review_text})
    df_reviews = pd.DataFrame(reviews_list)
    
    # --- 3. Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑù Î∞è ÏòàÏ∏° Î™®Îç∏ ÌïôÏäµÏö© df_products ÏÉùÏÑ± ---
    # Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞Î•º Ï†úÏô∏Ìïú ÏàúÏàò ÏÉÅÌíà Ï†ïÎ≥¥(Î©îÌÉÄÎç∞Ïù¥ÌÑ∞)ÎßåÏúºÎ°ú DataFrameÏùÑ Íµ¨ÏÑ±ÌïòÍ≥†, ÏÉÅÌíà ID Í∏∞Ï§Ä Ï§ëÎ≥µÏùÑ Ï†úÍ±∞Ìï©ÎãàÎã§.
    df_meta_cols = ['product_id', 'product_name', 'about_product', 'category_cleaned', 'discounted_price', 'rating_count', 'rating']
    df_products = df_raw[df_meta_cols].drop_duplicates(subset=['product_id']).copy() # type: ignore
    df_products.dropna(subset=['discounted_price', 'rating_count', 'rating', 'category_cleaned'], inplace=True)

    # --- 4. Î™®Îç∏ ÌïôÏäµ ---
    # Í∞ÄÍ≤©(numeric)Í≥º Ïπ¥ÌÖåÍ≥†Î¶¨(categorical) Ï†ïÎ≥¥Î•º Í∏∞Î∞òÏúºÎ°ú Î≥ÑÏ†êÍ≥º Î¶¨Î∑∞ ÏàòÎ•º ÏòàÏ∏°ÌïòÎäî Î™®Îç∏ÏùÑ ÎßåÎì≠ÎãàÎã§.
    numeric_features = ['discounted_price']
    categorical_features = ['category_cleaned']
    X_features = df_products[numeric_features + categorical_features]
    preprocessor = ColumnTransformer(transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)])
    
    # Î™®Îç∏ 1: Î≥ÑÏ†ê(rating) ÏòàÏ∏°
    y_rating = df_products['rating']
    ml_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', Ridge(alpha=1.0))])
    ml_pipe.fit(X_features, y_rating)
    print("‚úÖ Rating Prediction model training complete!")

    # Î™®Îç∏ 2: Î¶¨Î∑∞ Ïàò(rating_count) ÏòàÏ∏°
    y_review_count = df_products['rating_count']
    review_count_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', Ridge(alpha=1.0))])
    review_count_pipe.fit(X_features, y_review_count)
    print("‚úÖ Review Count Prediction model training complete!")

    # Î™®Îç∏ 3: TF-IDF Î™®Îç∏ ÌïôÏäµ (Ïú†ÏÇ¨ÎèÑ Î∂ÑÏÑùÏö©)
    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
    tfidf_matrix = tfidf_vectorizer.fit_transform(df_products['about_product']) # type: ignore
    print("‚úÖ TF-IDF model (based on product description) training complete!")
    
    # --- 5. Í≥ÑÏ∏µÏ†Å Ïπ¥ÌÖåÍ≥†Î¶¨ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ---
    # ÌîÑÎ°†Ìä∏ÏóîÎìúÏóêÏÑú ÎèôÏ†Å Ïπ¥ÌÖåÍ≥†Î¶¨ ÏÑ†ÌÉù UIÎ•º Íµ¨ÌòÑÌïòÍ∏∞ ÏúÑÌï¥
    temp_hierarchical_data = {}
    for cat_string in df_products['category_cleaned'].unique():
        parts = cat_string.split(' | ')
        current_level = temp_hierarchical_data
        for part in parts:
            if part not in current_level:
                current_level[part] = {}
            current_level = current_level[part]
    hierarchical_categories_data = temp_hierarchical_data
    print("‚úÖ Hierarchical category data created!")
    print(f"üìà Total {len(df_products)} unique products and {len(df_reviews)} individual reviews loaded.")
    print(f"‚≠ê Rating range found in data: {df_products['rating'].min()} ~ {df_products['rating'].max()}")


# --- ÏÑúÎ≤Ñ ÏãúÏûë Ïãú Ïã§ÌñâÎê† Î°úÏßÅ ---
@app.on_event("startup")
def startup_event():
    try:
        load_data_and_train_models()
    except Exception as e:
        print(f"üö® ÏÑúÎ≤Ñ ÏãúÏûë Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")


# --- API ÏóîÎìúÌè¨Ïù∏Ìä∏ Ï†ïÏùò ---
@app.get("/")
def read_root():
    """API ÏÑúÎ≤ÑÏùò Î£®Ìä∏ ÏóîÎìúÌè¨Ïù∏Ìä∏. ÏÑúÎ≤ÑÍ∞Ä Ïã§Ìñâ Ï§ëÏù∏ÏßÄ ÌôïÏù∏ÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§."""
    return {"message": "Rmazon predictor and similarity API is running!"}

@app.get("/categories", response_model=List[str])
def get_categories():
    """Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÏûàÎäî Î™®Îì† ÏµúÏÉÅÏúÑ Ïπ¥ÌÖåÍ≥†Î¶¨ Î™©Î°ùÏùÑ Î∞òÌôòÌï©ÎãàÎã§."""
    if not hierarchical_categories_data:
        raise HTTPException(status_code=503, detail="Ïπ¥ÌÖåÍ≥†Î¶¨ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏïÑÏßÅ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    return list(hierarchical_categories_data.keys())

@app.get("/hierarchical-categories", response_model=Dict)
def get_hierarchical_categories():
    """Ï†ÑÏ≤¥ Ïπ¥ÌÖåÍ≥†Î¶¨ Íµ¨Ï°∞Î•º Í≥ÑÏ∏µÏ†Å(JSON) ÌòïÌÉúÎ°ú Î∞òÌôòÌï©ÎãàÎã§."""
    if not hierarchical_categories_data:
        raise HTTPException(status_code=503, detail="Ïπ¥ÌÖåÍ≥†Î¶¨ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏïÑÏßÅ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    return hierarchical_categories_data

@app.get("/product-count", response_model=int)
def get_product_count(category: str = Query(..., description="ÏÉÅÌíà ÏàòÎ•º Ï°∞ÌöåÌï† Ï†ÑÏ≤¥ Ïπ¥ÌÖåÍ≥†Î¶¨ Í≤ΩÎ°ú")):
    """ÏÑ†ÌÉùÎêú ÏµúÏ¢Ö Ïπ¥ÌÖåÍ≥†Î¶¨Ïóê Ìï¥ÎãπÌïòÎäî ÏÉÅÌíàÏùò Ï¥ù Í∞úÏàòÎ•º Î∞òÌôòÌï©ÎãàÎã§."""
    if df_products.empty:
        return 0
    return int(df_products[df_products['category_cleaned'] == category].shape[0])

@app.get("/products", response_model=List[Product])
def get_products(category: Optional[str] = Query(None)):
    """ÌäπÏ†ï Ïπ¥ÌÖåÍ≥†Î¶¨Ïóê ÏÜçÌïú ÏÉÅÌíà Î™©Î°ùÏùÑ Î∞òÌôòÌï©ÎãàÎã§."""
    if df_products.empty:
        return []
    target_df = df_products
    if category:
        target_df = df_products[df_products['category_cleaned'] == category]
    return target_df[['product_id', 'product_name']].to_dict(orient='records') # type: ignore

def advanced_review_analysis(reviews: List[str]) -> Dict[str, Any]:
    """
    VADERÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Î¶¨Î∑∞ Î™©Î°ùÏùò Í∏ç/Î∂ÄÏ†ï/Ï§ëÎ¶Ω ÎπÑÏú®ÏùÑ Î∂ÑÏÑùÌïòÍ≥†,
    Í∏çÏ†ï/Î∂ÄÏ†ï Î¶¨Î∑∞ÏóêÏÑú Í∞ÄÏû• ÎπàÎèÑÍ∞Ä ÎÜíÏùÄ ÌÇ§ÏõåÎìúÎ•º Ï∂îÏ∂úÌï©ÎãàÎã§.
    """
    if not reviews:
        return {"positive_percentage": 0, "negative_percentage": 0, "neutral_percentage": 100, "top_positive_keywords": [], "top_negative_keywords": []}
    
    analyzer = SentimentIntensityAnalyzer()
    positive_reviews_text, negative_reviews_text = [], []
    pos_count, neg_count, neu_count = 0, 0, 0

    for review in reviews:
        score = analyzer.polarity_scores(review)
        if score['compound'] >= 0.05:
            positive_reviews_text.append(review)
            pos_count += 1
        elif score['compound'] <= -0.05:
            negative_reviews_text.append(review)
            neg_count += 1
        else:
            neu_count += 1

    total = len(reviews)
    positive_percentage = (pos_count / total) * 100
    negative_percentage = (neg_count / total) * 100
    neutral_percentage = (neu_count / total) * 100

    custom_stop_words = text.ENGLISH_STOP_WORDS.union(['product', 'good', 'great', 'bad', 'price', 'quality', 'item', 'like', 'just', 'really', 'very', 'amazon', 'nice', 'best', 'time', 'use', 'working', 'not', 'don', 'doesn', 'is', 'are'])

    def extract_top_keywords(texts: List[str], stop_words, top_n=5) -> List[str]:
        if not texts: return []
        words = ' '.join(texts).lower().split()
        filtered_words = [word for word in words if word.isalpha() and word not in stop_words]
        return [word for word, count in Counter(filtered_words).most_common(top_n)]

    positive_keywords = extract_top_keywords(positive_reviews_text, custom_stop_words)
    negative_keywords = extract_top_keywords(negative_reviews_text, custom_stop_words)

    return {
        'positive_percentage': round(positive_percentage, 2),
        'negative_percentage': round(negative_percentage, 2),
        'neutral_percentage': round(neutral_percentage, 2),
        'top_positive_keywords': positive_keywords,
        'top_negative_keywords': negative_keywords
    }

@app.get("/category-price-range", response_model=PriceRangeResponse)
def get_category_price_range(category: str = Query(...)):
    """ÌäπÏ†ï Ïπ¥ÌÖåÍ≥†Î¶¨Ïùò ÏµúÏÜå/ÏµúÎåÄ Í∞ÄÍ≤©ÏùÑ Î∞òÌôòÌï©ÎãàÎã§."""
    if df_products.empty:
        raise HTTPException(status_code=404, detail="ÏÉÅÌíà Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
    filtered_df = df_products[df_products['category_cleaned'] == category]
    if filtered_df.empty:
        return PriceRangeResponse(min_price=0.0, max_price=0.0)
    min_price = filtered_df['discounted_price'].min()
    max_price = filtered_df['discounted_price'].max()
    return PriceRangeResponse(min_price=float(min_price), max_price=float(max_price))

@app.get("/category-stats", response_model=CategoryStatsResponse)
def get_category_stats(category: str = Query(..., description="ÌÜµÍ≥Ñ Ï†ïÎ≥¥Î•º Ï°∞ÌöåÌï† Ïπ¥ÌÖåÍ≥†Î¶¨")):
    """ÌäπÏ†ï Ïπ¥ÌÖåÍ≥†Î¶¨Ïùò Í∞ÄÍ≤©, Î¶¨Î∑∞ Ïàò, Î≥ÑÏ†ê Î∂ÑÌè¨ Îì± Ï¢ÖÌï©Ï†ÅÏù∏ ÌÜµÍ≥Ñ Ï†ïÎ≥¥Î•º Î∞òÌôòÌï©ÎãàÎã§."""
    if df_products.empty:
        raise HTTPException(status_code=503, detail="ÏÉÅÌíà Îç∞Ïù¥ÌÑ∞Í∞Ä ÏïÑÏßÅ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    
    category_df = df_products[df_products['category_cleaned'] == category]
    if category_df.empty:
        return CategoryStatsResponse(min_price=0, max_price=0, min_review_count=0, max_review_count=0, price_distribution=[], review_count_distribution=[], rating_distribution=[])

    def create_histogram(data: pd.Series, bins=10) -> List[DistributionBin]:
        if data.empty: return []
        counts, bin_edges = np.histogram(data.dropna(), bins=bins)
        return [DistributionBin(name=f"{bin_edges[i]:.0f}-{bin_edges[i+1]:.0f}", count=int(counts[i])) for i in range(len(counts))]

    def process_rating_distribution(data: pd.Series) -> List[DistributionBin]:
        if data.empty: return []
        rating_counts = (data.dropna() / 0.5).round() * 0.5
        rating_distribution = rating_counts.value_counts().sort_index()
        return [DistributionBin(name=f"{rating:.1f}", count=int(count)) for rating, count in rating_distribution.items()]

    price_dist = create_histogram(category_df['discounted_price']) # type: ignore
    review_count_dist = create_histogram(category_df['rating_count']) # type: ignore
    rating_dist = process_rating_distribution(category_df['rating']) # type: ignore

    return CategoryStatsResponse(
        min_price=float(category_df['discounted_price'].min()), # type: ignore
        max_price=float(category_df['discounted_price'].max()), # type: ignore
        min_review_count=float(category_df['rating_count'].min()), # type: ignore
        max_review_count=float(category_df['rating_count'].max()), # type: ignore
        price_distribution=price_dist,
        review_count_distribution=review_count_dist,
        rating_distribution=rating_dist
    )

@app.post("/search-similarity", response_model=List[SimilarityResult])
def search_similarity(request: SimilarityRequest):
    """
    ÏÇ¨Ïö©Ïûê ÏûÖÎ†•ÏùÑ Í∏∞Î∞òÏúºÎ°ú 3Í∞ÄÏßÄ ÏöîÏÜåÎ•º Ï¢ÖÌï©ÌïòÏó¨ Ïú†ÏÇ¨ ÏÉÅÌíàÏùÑ Í≤ÄÏÉâÌï©ÎãàÎã§.
    [ÌòÑÏû¨ ÎîîÎ≤ÑÍπÖ Ï§ë] - ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ(TF-IDF)ÎßåÏúºÎ°ú Ï†êÏàòÎ•º Í≥ÑÏÇ∞ÌïòÏó¨ Î¨∏Ï†úÏùò ÏõêÏù∏ÏùÑ Í≤©Î¶¨Ìï©ÎãàÎã§.
    """
    if df_products.empty or tfidf_vectorizer is None or tfidf_matrix is None:
        raise HTTPException(status_code=503, detail="ÏÑúÎ≤ÑÍ∞Ä ÏïÑÏßÅ Ï§ÄÎπÑÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.")

    # 1. ÌäπÏ†ï Ïπ¥ÌÖåÍ≥†Î¶¨Ïùò ÏÉÅÌíàÎßå ÌïÑÌÑ∞ÎßÅ
    df_filtered = df_products[df_products['category_cleaned'] == request.category].copy()
    if df_filtered.empty:
        return []

    # --- Î¨∏Ï†ú Í≤©Î¶¨Î•º ÏúÑÌï¥ ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑÎßå Í≥ÑÏÇ∞ ---
    filtered_indices = df_filtered.index.tolist()
    filtered_tfidf_matrix = tfidf_matrix[filtered_indices]

    user_desc_vector = tfidf_vectorizer.transform([request.description])
    text_similarities = cosine_similarity(user_desc_vector, filtered_tfidf_matrix).flatten()
    
    # ÏµúÏ¢Ö Ïú†ÏÇ¨ÎèÑÎ•º Ïò§ÏßÅ ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑÎßåÏúºÎ°ú ÏÑ§Ï†ï
    df_filtered['similarity'] = text_similarities * 100

    # ÏÉÅÏúÑ 10Í∞ú Í≤∞Í≥º Ï†ïÎ†¨ Î∞è ÏÑ†ÌÉù
    top_10_similar_products = df_filtered.sort_values(by='similarity', ascending=False).head(10) # type: ignore
    
    results = []
    for _, row in top_10_similar_products.iterrows():
        product_reviews = df_reviews[df_reviews['product_id'] == row['product_id']]['review_text'].tolist()
        review_analysis_data = advanced_review_analysis(product_reviews)
        
        results.append(SimilarityResult(
            product_id=str(row['product_id']),
            product_name=str(row['product_name']),
            category=str(row['category_cleaned']),
            similarity=round(float(row['similarity']), 2),
            discounted_price=float(row['discounted_price']),
            rating=float(row['rating']),
            rating_count=int(row['rating_count']),
            review_analysis=ReviewAnalysis(**review_analysis_data)
        ))
        
    return results

@app.post("/predict", response_model=PredictionResponse)
def predict_star_rating(request: PredictionRequest):
    """
    ÏûÖÎ†•Îêú Í∞ÄÍ≤©Í≥º Ïπ¥ÌÖåÍ≥†Î¶¨Î•º Î∞îÌÉïÏúºÎ°ú ÏòàÏÉÅ Î≥ÑÏ†êÍ≥º Î¶¨Î∑∞ ÏàòÎ•º ÏòàÏ∏°Ìï©ÎãàÎã§.
    ÎòêÌïú ÏûÖÎ†•Îêú Í∞ÄÍ≤©Ïù¥ Ìï¥Îãπ Ïπ¥ÌÖåÍ≥†Î¶¨ ÎÇ¥ÏóêÏÑú Ïñ¥Îäê Ï†ïÎèÑ ÏàòÏ§ÄÏù∏ÏßÄ Î∞±Î∂ÑÏúÑ Ï†ïÎ≥¥Î•º Ìï®Íªò Ï†úÍ≥µÌï©ÎãàÎã§.
    """
    if ml_pipe is None or df_products.empty or review_count_pipe is None:
        raise HTTPException(status_code=503, detail="Î™®Îç∏Ïù¥ ÏïÑÏßÅ Ï§ÄÎπÑÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    
    try:
        input_data = pd.DataFrame([[request.price, request.category]], columns=['discounted_price', 'category_cleaned']) # type: ignore
        predicted_star = ml_pipe.predict(input_data)[0]
        predicted_review_count = review_count_pipe.predict(input_data)[0]

        category_df = df_products[df_products['category_cleaned'] == request.category]
        
        def calculate_percentile(series: pd.Series, score: float) -> float:
            if series.empty: return 0.0
            from scipy.stats import percentileofscore
            return float(percentileofscore(series.dropna(), score, kind='weak'))

        price_percentile = calculate_percentile(category_df['discounted_price'], request.price) # type: ignore
        review_count_percentile = calculate_percentile(category_df['rating_count'], float(predicted_review_count)) # type: ignore
        rating_percentile = calculate_percentile(category_df['rating'], float(predicted_star)) # type: ignore

        return PredictionResponse(
            predicted_star=round(float(predicted_star), 2),
            predicted_review_count=round(float(predicted_review_count), 0),
            price_percentile=round(price_percentile, 2),
            review_count_percentile=round(review_count_percentile, 2),
            rating_percentile=round(rating_percentile, 2)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÏòàÏ∏° Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}") 