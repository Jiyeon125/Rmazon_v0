from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import text
import os
import shutil
from typing import List, Optional, Dict
import numpy as np
import math

# --- ìƒìˆ˜ ì •ì˜ ---
MIN_RATING = 1.0
MAX_RATING = 5.0

# --- Pydantic ëª¨ë¸ ì •ì˜ ---
# ìš”ì²­ ë³¸ë¬¸ì˜ ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class Keyword(BaseModel):
    word: str
    count: int

class PredictionRequest(BaseModel):
    price: float
    review_count: int
    category: str

# ì‘ë‹µ ë³¸ë¬¸ì˜ ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class PredictionResponse(BaseModel):
    predicted_star: float

class SimilarityRequest(BaseModel):
    description: str
    price: float
    discount_percentage: float
    category: str

class Product(BaseModel):
    product_id: str
    product_name: str

class ReviewAnalysis(BaseModel):
    overall_sentiment: str
    sentiment_distribution: Dict[str, int]
    top_keywords: List[Keyword]
    negative_concerns: List[str]
    summary: str
    review_count: int

class SimilarityResult(BaseModel):
    product_id: str
    product_name: str
    similarity: float
    discounted_price: float
    rating: float
    rating_count: int
    review_analysis: ReviewAnalysis

class PriceRangeResponse(BaseModel):
    min_price: float
    max_price: float

# --- FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì • ---
app = FastAPI()

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€: Next.js ì•±(http://localhost:3000)ì—ì„œì˜ ìš”ì²­ì„ í—ˆìš©í•©ë‹ˆë‹¤.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ì „ì—­ ë³€ìˆ˜: ëª¨ë¸, ë°ì´í„°, ì „ì²˜ë¦¬ê¸° ---
ml_pipe = None
tfidf_vectorizer = None
tfidf_matrix = None
df_products = pd.DataFrame() # ìƒí’ˆ ë©”íƒ€ë°ì´í„° ë° ìœ ì‚¬ë„ ë¶„ì„ìš©
df_reviews = pd.DataFrame() # ìƒí’ˆë³„ ê°œë³„ ë¦¬ë·° ì €ì¥ìš©
DATA_FILE_PATH = os.path.join("data", "cleaned_amazon_0519.csv")

# --- í•µì‹¬ ë¡œì§: ë°ì´í„° ë¡œë”© ë° ëª¨ë¸ í•™ìŠµ ---
def load_data_and_train_models():
    global ml_pipe, tfidf_vectorizer, tfidf_matrix, df_products, df_reviews
    
    if not os.path.exists(DATA_FILE_PATH):
        print(f"âš ï¸ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {DATA_FILE_PATH}")
        df_products, df_reviews = pd.DataFrame(), pd.DataFrame()
        return

    df_raw = pd.read_csv(DATA_FILE_PATH)
    
    required_columns = ['product_id', 'product_name', 'review_title', 'review_content', 'discounted_price', 'rating_count', 'category_cleaned', 'rating']
    if any(col not in df_raw.columns for col in required_columns):
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ì¤‘ ì¼ë¶€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- 1. ê¸°ë³¸ í´ë¦¬ë‹ ë° íƒ€ì… ë³€í™˜ ---
    df_raw['review_title'] = df_raw['review_title'].fillna('')
    df_raw['review_content'] = df_raw['review_content'].fillna('')
    for col in ['discounted_price', 'rating_count', 'rating']:
        df_raw[col] = pd.to_numeric(df_raw[col], errors='coerce')
    df_raw.dropna(subset=['product_id', 'discounted_price', 'rating_count', 'rating', 'category_cleaned'], inplace=True)

    # --- 2. ë¦¬ë·° ë¶„ë¦¬ ë° df_reviews ìƒì„± ---
    reviews_list = []
    for _, row in df_raw.iterrows():
        # review_contentë¥¼ ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ ê°œë³„ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ ìƒì„±
        # ë‚´ìš©ì´ ì—†ëŠ” ë¹ˆ ë¦¬ë·°ëŠ” ì œì™¸
        contents = [c.strip() for c in str(row['review_content']).split(',') if c.strip()]
        for content_part in contents:
            reviews_list.append({
                'product_id': row['product_id'],
                'review_text': content_part
            })
    
    df_reviews = pd.DataFrame(reviews_list)
    
    if df_reviews.empty:
        print("âš ï¸ ë¦¬ë·° ë°ì´í„°ë¥¼ ë¶„ë¦¬í•œ í›„ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        df_products = pd.DataFrame()
        return

    # --- 3. ìœ ì‚¬ë„ ë¶„ì„ìš© df_products ìƒì„± ---
    # ìƒí’ˆë³„ë¡œ ë¶„ë¦¬ëœ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ í•˜ë‚˜ë¡œ í•©ì³ 'combined_text' ìƒì„±
    df_aggregated_reviews = df_reviews.groupby('product_id')['review_text'].apply(lambda x: ' '.join(x)).reset_index()
    df_aggregated_reviews.rename(columns={'review_text': 'combined_text'}, inplace=True)

    # ì›ë³¸ ë°ì´í„°ì—ì„œ ìƒí’ˆ ë©”íƒ€ë°ì´í„°(ë¦¬ë·° ì œì™¸)ë¥¼ ê°€ì ¸ì™€ ê²°í•©
    df_meta = df_raw.drop(columns=['review_title', 'review_content']).drop_duplicates(subset=['product_id']).set_index('product_id')
    df_products = df_aggregated_reviews.merge(df_meta, on='product_id', how='left')
    
    # ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ëª¨ë‘ ìˆëŠ”ì§€ ìµœì¢… í™•ì¸
    df_products.dropna(subset=['discounted_price', 'rating_count', 'rating', 'category_cleaned'], inplace=True)
    
    if df_products.empty:
        print("âš ï¸ ìµœì¢… ìƒí’ˆ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
        
    # --- 4. ëª¨ë¸ í•™ìŠµ ---
    # ë¦¿ì§€ íšŒê·€ ëª¨ë¸ í•™ìŠµ (ë³„ì  ì˜ˆì¸¡ìš©)
    X_ridge = df_products[['discounted_price', 'rating_count', 'category_cleaned']]
    y_ridge = df_products['rating']
    numeric_features = ['discounted_price', 'rating_count']
    categorical_features = ['category_cleaned']
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numeric_features),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)])
    ml_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', Ridge(alpha=1.0))])
    ml_pipe.fit(X_ridge, y_ridge)
    print("âœ… Ridge Regression model training complete!")

    # TF-IDF ëª¨ë¸ í•™ìŠµ (ìœ ì‚¬ë„ ë¶„ì„ìš©)
    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
    tfidf_matrix = tfidf_vectorizer.fit_transform(df_products['combined_text'])
    print("âœ… TF-IDF model training complete!")
    print(f"ğŸ“ˆ Total {len(df_products)} unique products and {len(df_reviews)} individual reviews loaded.")


# --- ì„œë²„ ì‹œì‘ ì‹œ ì‹¤í–‰ë  ë¡œì§ ---
@app.on_event("startup")
def startup_event():
    try:
        load_data_and_train_models()
    except Exception as e:
        print(f"ğŸš¨ ì„œë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# --- API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ ---
@app.get("/")
def read_root():
    return {"message": "Rmazon predictor and similarity API is running!"}

@app.post("/upload-csv")
async def upload_csv(file: UploadFile = File(...)):
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    temp_file_path = f"temp_{file.filename}"
    with open(temp_file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    # ìƒˆ íŒŒì¼ ê²€ì¦
    try:
        df_new = pd.read_csv(temp_file_path)
        required_columns = ['product_id', 'product_name', 'review_title', 'review_content', 'discounted_price', 'rating_count', 'category_cleaned', 'rating']
        missing = [col for col in required_columns if col not in df_new.columns]
        if missing:
            raise HTTPException(status_code=400, detail=f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {', '.join(missing)}")
        
        # ê²€ì¦ í†µê³¼ ì‹œ, ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°
        shutil.move(temp_file_path, DATA_FILE_PATH)
        
        # ë°ì´í„°ì™€ ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œ
        load_data_and_train_models()
        
        return {"message": "íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œ ë° ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.", "rows": len(df_new)}
    
    except Exception as e:
        os.remove(temp_file_path) # ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ ì œê±°
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        await file.close()

@app.get("/categories", response_model=List[str])
def get_categories():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df_products.empty:
        return []
    return sorted(df_products['category_cleaned'].unique().tolist())

@app.get("/products", response_model=List[Product])
def get_products(category: Optional[str] = Query(None)):
    """
    ìƒí’ˆ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - category ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ìƒí’ˆë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    if df_products.empty:
        return []
    
    if category:
        filtered_df = df_products[df_products['category_cleaned'] == category]
        return filtered_df[['product_id', 'product_name']].to_dict('records')
    
    # ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´ ì „ì²´ ëª©ë¡ ë°˜í™˜ (ê´€ë¦¬ìš© ë˜ëŠ” ë‹¤ë¥¸ ìš©ë„ë¡œ ìœ ì§€)
    return df_products[['product_id', 'product_name']].to_dict('records')

# --- ê³ ê¸‰ ë¦¬ë·° ë¶„ì„ ë¡œì§ (ì„œë²„ ì‚¬ì´ë“œë¡œ ì´ë™) ---
def advanced_review_analysis(reviews: List[str]) -> Dict:
    # ì´ ë¶€ë¶„ì€ ì´ì „ì— í”„ë¡ íŠ¸ì—”ë“œì— ìˆë˜ ë¡œì§ì„ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤.
    # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬(spaCy, NLTK ë“±)ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ,
    # ê¸°ì¡´ ê¸°ëŠ¥ ë³µì›ì„ ìœ„í•´ ë™ì¼í•œ ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    # ... (ì—¬ê¸°ì— ê°ì„±ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ ë“± ê¸°ì¡´ ë¡œì§ êµ¬í˜„) ...
    # ê°„ë‹¨í•œ êµ¬í˜„ ì˜ˆì‹œ:
    positive_words = ['good', 'great', 'excellent', 'love', 'best']
    negative_words = ['bad', 'poor', 'terrible', 'hate', 'worst']
    
    sentiments = {'positive': 0, 'neutral': 0, 'negative': 0}
    all_words = []
    
    for review in reviews:
        review_lower = review.lower()
        pos_count = sum(1 for word in positive_words if word in review_lower)
        neg_count = sum(1 for word in negative_words if word in review_lower)
        
        if pos_count > neg_count:
            sentiments['positive'] += 1
        elif neg_count > pos_count:
            sentiments['negative'] += 1
        else:
            sentiments['neutral'] += 1
        
        all_words.extend(review_lower.split())

    # ì „ì²´ ê°ì„±
    overall = max(sentiments, key=sentiments.get)

    # í‚¤ì›Œë“œ (ê°„ë‹¨í•œ ë¹ˆë„ìˆ˜ ê¸°ë°˜)
    from collections import Counter
    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ í™•ì¥
    stop_words_list = list(text.ENGLISH_STOP_WORDS) + ['product', 'amazon', 'use', 'get', 'it', 'i']
    
    keywords_with_counts = [
        (word, count) for word, count in Counter(all_words).most_common(20) 
        if word.isalpha() and len(word) > 2 and word not in stop_words_list
    ]

    return {
        "overall_sentiment": overall,
        "sentiment_distribution": sentiments,
        "top_keywords": [{"word": w, "count": c} for w, c in keywords_with_counts[:5]], # ìƒìœ„ 5ê°œë§Œ ì„ íƒ
        "negative_concerns": [r for r in reviews if any(w in r.lower() for w in negative_words)][:2],
        "summary": f"ì „ì²´ì ìœ¼ë¡œ {overall}ì ì¸ í‰ê°€ê°€ ë§ìŠµë‹ˆë‹¤. ì£¼ìš” í‚¤ì›Œë“œëŠ” {', '.join([k[0] for k in keywords_with_counts[:5]])} ë“±ì…ë‹ˆë‹¤.",
        "review_count": len(reviews)
    }

def squash_to_rating_range(x: float, center: float, scale: float = 0.5) -> float:
    """
    ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ë³€í˜•í•˜ì—¬ ì…ë ¥ê°’ xë¥¼ [MIN_RATING, MAX_RATING] ë²”ìœ„ë¡œ ë¶€ë“œëŸ½ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
    - center: ë³€í™˜ì˜ ì¤‘ì‹¬ì´ ë˜ëŠ” ê°’ (ë°ì´í„°ì˜ í‰ê·  ë³„ì  ë“±)
    - scale: ê³¡ì„ ì˜ ê°€íŒŒë¥¸ ì •ë„ë¥¼ ì¡°ì ˆ
    """
    k = scale
    return MIN_RATING + (MAX_RATING - MIN_RATING) / (1 + math.exp(-k * (x - center)))

@app.get("/category-price-range", response_model=PriceRangeResponse)
def get_category_price_range(category: str = Query(...)):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ìµœì†Œ ë° ìµœëŒ€ ê°€ê²©ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df_products.empty:
        raise HTTPException(status_code=503, detail="ì„œë²„ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    filtered_df = df_products[df_products['category_cleaned'] == category]
    
    if filtered_df.empty:
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ìƒí’ˆì´ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„°ì˜ ê°€ê²© ë²”ìœ„ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì œê³µ
        min_price = df_products['discounted_price'].min()
        max_price = df_products['discounted_price'].max()
    else:
        min_price = filtered_df['discounted_price'].min()
        max_price = filtered_df['discounted_price'].max()

    return {"min_price": min_price, "max_price": max_price}

@app.post("/search-similarity", response_model=List[SimilarityResult])
def search_similarity(request: SimilarityRequest):
    """
    ì…ë ¥ëœ ìƒí’ˆ ì •ë³´ì™€ ê°€ì¥ ìœ ì‚¬í•œ ìƒí’ˆ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìœ ì‚¬ë„ëŠ” í…ìŠ¤íŠ¸(TF-IDF)ì™€ ê°€ê²©ì„ ì¢…í•©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤.
    ê° ìœ ì‚¬ ìƒí’ˆì— ëŒ€í•´ ê°œë³„ì ì¸ ë¦¬ë·° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if df_products.empty or tfidf_matrix is None:
        raise HTTPException(status_code=503, detail="ì„œë²„ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    if not request.description.strip() or not request.category:
        raise HTTPException(status_code=400, detail="ìƒí’ˆ ì„¤ëª…ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (TF-IDF)
    input_vector = tfidf_vectorizer.transform([request.description])
    text_similarities = cosine_similarity(input_vector, tfidf_matrix).flatten()

    # 2. ì¹´í…Œê³ ë¦¬ê°€ ì¼ì¹˜í•˜ëŠ” ìƒí’ˆë§Œ í•„í„°ë§
    category_mask = df_products['category_cleaned'] == request.category
    
    # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
    # ê°€ê²© ìœ ì‚¬ë„: ìš”ì²­ ê°€ê²©ê³¼ì˜ ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ë†’ìŒ (ì •ê·œí™”)
    price_diff = np.abs(df_products['discounted_price'] - request.price)
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì•„ì£¼ ì‘ì€ ê°’(epsilon)ì„ ë”í•¨
    price_similarity = 1 - (price_diff / (price_diff.max() + 1e-6))
    
    # ì¢…í•© ì ìˆ˜ = í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ * 0.7 + ê°€ê²© ìœ ì‚¬ë„ * 0.3
    combined_scores = (text_similarities * 0.7) + (price_similarity * 0.3)
    
    # ì¹´í…Œê³ ë¦¬ ë§ˆìŠ¤í¬ ì ìš©
    combined_scores[~category_mask] = 0

    # 4. ìƒìœ„ 5ê°œ ìƒí’ˆ ì„ ì •
    top_n = 5
    # ì ìˆ˜ê°€ 0ì¸ ê²½ìš°ëŠ” ì œì™¸í•˜ê³ , ìƒìœ„ Nê°œë¥¼ ì°¾ìŒ
    valid_scores_indices = np.where(combined_scores > 0)[0]
    if len(valid_scores_indices) == 0:
        return []
    
    top_indices = valid_scores_indices[np.argsort(combined_scores[valid_scores_indices])[-top_n:]][::-1]

    # 5. ìµœì¢… ê²°ê³¼ ìƒì„± (ìƒí’ˆë³„ ê°œë³„ ë¶„ì„)
    results = []
    for idx in top_indices:
        product_row = df_products.iloc[idx]
        product_id = product_row['product_id']
        
        # ìƒí’ˆë³„ ê°œë³„ ë¦¬ë·° ì¶”ì¶œ
        product_reviews = df_reviews[df_reviews['product_id'] == product_id]['review_text'].tolist()
        
        # ìƒí’ˆë³„ ë¦¬ë·° ë¶„ì„ ìˆ˜í–‰
        if not product_reviews:
            # ë¦¬ë·°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
            review_analysis_result = {
                'overall_sentiment': 'neutral',
                'sentiment_distribution': {'positive': 0, 'neutral': 0, 'negative': 0},
                'top_keywords': [], 'negative_concerns': [],
                'summary': 'ë¦¬ë·° ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
                'review_count': 0
            }
        else:
            review_analysis_result = advanced_review_analysis(product_reviews)
            
        results.append(SimilarityResult(
            product_id=product_id,
            product_name=product_row['product_name'],
            similarity=combined_scores[idx],
            discounted_price=product_row['discounted_price'],
            rating=product_row['rating'],
            rating_count=product_row['rating_count'],
            review_analysis=review_analysis_result
        ))
        
    return results

@app.post("/predict", response_model=PredictionResponse)
def predict_star_rating(request: PredictionRequest):
    if ml_pipe is None or df_products.empty:
        raise HTTPException(status_code=503, detail="ì˜ˆì¸¡ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    input_data_dict = {
        'discounted_price': request.price,
        'rating_count': request.review_count,
        'category_cleaned': request.category
    }
    input_data = pd.DataFrame([input_data_dict])
    
    predicted_star = ml_pipe.predict(input_data)[0]
    
    # ì˜ˆì¸¡ ê²°ê³¼ì˜ ì¤‘ì‹¬ì ì„ ë°ì´í„°ì…‹ì˜ í‰ê·  ë³„ì ìœ¼ë¡œ ì‚¬ìš©
    rating_center = df_products['rating'].mean()

    # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ 1~5ì  ì‚¬ì´ë¡œ ì •ê·œí™”
    normalized_star = squash_to_rating_range(predicted_star, center=rating_center)
    
    return {"predicted_star": round(normalized_star, 2)} 